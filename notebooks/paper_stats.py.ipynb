{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import csv\n",
    "from textstat.textstat import *\n",
    "from vaderSentiment.vaderSentiment import sentiment as VS\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import metrics\n",
    "from sklearn import naive_bayes\n",
    "from sklearn import cross_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##Defining stopwords list\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "stemmer = nltk.stem.porter.PorterStemmer()\n",
    "\n",
    "def tokenize(page):\n",
    "    tokens = page.split()\n",
    "    #Remove stopwords\n",
    "    #tokens = [t for t in tokens if t not in stopwords]\n",
    "    #stem tokens\n",
    "    tokens = [stemmer.stem(t) for t in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m1 = \"The mid 2010s have witnessed a resurgence of nationalist discourse in the United States, mirroring longer-term trends in the European public sphere. Politicians on both sides of the Atlantic have articulated visions of their nations under siege—by immigrants, refugees, domestic minority pop- ulations, and these groups’ ostensible accomplices among the political and cultural elites. Evoking nostalgia for the nation’s bygone glory days, these diagnoses have been coupled with sundry pol- icy proposals aimed at making the country great again, to paraphrase Donald Trump’s campaign slogan: from the tightening of national borders, increased surveillance of national populations, and scaling back of supranational integration to an ill-fitting mix of foreign policy isolationism and hawkish calls for unilateral projection of military power abroad. Narratives of the nation’s putative failings have resonated with beliefs deeply held by large segments of the voting public, laying bare cultural cleavages that are likely to shape election outcomes, policy decisions, and social movement mobilization.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m2 = \"What might a research program built on such a foundation look like? First, it should consider nationalism from the bottom up, as a set of intersubjective meanings and affective orientations that give people a sense of self and guide their social interactions and political choices. Such a shift would imply not only a focus on popular beliefs and attitudes, but also the understanding that nationhood is only one source of identity, whose salience depends on a variety of contextual factors. Second, such research should explicitly consider the heterogeneity of vernacular conceptions of the nation within any given polity. The nation is not a static cultural object with a single shared meaning, but a site of active political contestation between cultural communities with strikingly different belief systems. Such conflicts are at the heart of contemporary political debates in the United States and Europe.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m3 = \"All three objectives require scholars’ engagement with meanings held by individuals embedded in concrete social environments. If the nation is not just a political entity but also a cognitive frame through which people apprehend social reality and construct routinized strategies of action, research on nationalism must incorporate insights from cultural sociology and social psychology about how meanings structured by institutions shape social interaction and group relations. This suggests a research strategy that views dispositions toward the nation as relational, intersubjective, morally and affectively laden, and largely taken for granted. The resulting empirical investigations are likely to require an adaptation of existing research methods and the exploitation of new sources of data. Fortunately, the constitutive elements of this research agenda already exist; what is needed is their integration across disciplinary and methodological boundaries.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "papers = [m1,m2,m3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def word_count(paper):\n",
    "    paper = paper.split()\n",
    "    return len(paper)\n",
    "\n",
    "def char_count(word):\n",
    "    return len(word)\n",
    "\n",
    "def avg_word_length(paper):\n",
    "    words = word_count(paper)\n",
    "    p=paper.split()\n",
    "    char_total = 0\n",
    "    for word in p:\n",
    "        char_total += char_count(word)\n",
    "    return round(float(char_total)/float(words), 3)\n",
    "        \n",
    "def max_word_length(paper):\n",
    "    p=paper.split()\n",
    "    max_length = 0\n",
    "    for word in p:\n",
    "        if len(word) > max_length:\n",
    "            max_length = len(word)\n",
    "    return max_length\n",
    "\n",
    "def avg_syllables_per_word(paper):\n",
    "    return textstat.avg_syllables_per_word(paper)\n",
    "\n",
    "def avg_letters_per_word(paper):\n",
    "    return textstat.avg_letter_per_word(paper)\n",
    "\n",
    "def num_difficult_words(paper):\n",
    "    return textstat.difficult_words(paper)\n",
    "\n",
    "def num_polysyllable(paper):\n",
    "    return textstat.polysyllabcount(paper)\n",
    "\n",
    "def num_sentences(paper):\n",
    "    return textstat.sentence_count(paper)\n",
    "\n",
    "def ref_count_super_basic(paper):\n",
    "    return paper.count(')')\n",
    "\n",
    "def url_count(paper):\n",
    "    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
    "        '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    parsed_text = re.sub(giant_url_regex, 'URL', paper)\n",
    "    return paper.count('URL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def basic_stats(paper):\n",
    "    return [word_count(paper), avg_word_length(paper), max_word_length(paper), avg_syllables_per_word(paper) /\n",
    "            avg_letters_per_word(paper), num_difficult_words(paper), num_polysyllable(paper), num_sentences(paper)] #/\n",
    "        \n",
    "          # ref_count_super_basic(paper), url_count(paper)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def readability(paper):\n",
    "    fkg = textstat.flesch_kincaid_grade(paper)\n",
    "    fre = textstat.flesch_reading_ease(paper)\n",
    "    dcr = textstat.dale_chall_readability_score(paper)\n",
    "    smg = textstat.smog_index(paper)\n",
    "    cli = textstat.coleman_liau_index(paper)\n",
    "    ari = textstat.automated_readability_index(paper)\n",
    "    return [fkg,fre,dcr,smg,cli,ari]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Get an SAT/GRE word list and use counts of the words as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def avg_readability(readability_scores):\n",
    "    readability = np.array(readability_scores)\n",
    "    mean = np.mean(readability)\n",
    "    sd = np.std(readability)\n",
    "    return [mean, sd]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "paper_stats = []\n",
    "for p in papers:\n",
    "    rd = readability(p)\n",
    "    avgs = avg_readability(rd)\n",
    "    bs = basic_stats(p)\n",
    "    stats = rd+avgs+bs\n",
    "    for i in range(0, len(stats)):\n",
    "        stats[i] = round(stats[i], 3)\n",
    "    paper_stats.append(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = np.array(paper_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  22.   ,    6.51 ,   12.96 ,   14.6  ,   18.81 ,   26.9  ,\n",
       "          16.963,    6.563,  156.   ,    6.122,   13.   ,    0.31 ,\n",
       "          73.   ,   16.   ,    4.   ],\n",
       "       [  13.4  ,   39.67 ,   10.23 ,   11.2  ,   14.45 ,   15.9  ,\n",
       "          17.475,   10.105,  142.   ,    5.415,   15.   ,    0.314,\n",
       "          49.   ,   16.   ,    6.   ],\n",
       "       [  18.2  ,   11.25 ,   12.1  ,   15.9  ,   19.61 ,   21.5  ,\n",
       "          16.427,    3.76 ,  132.   ,    6.311,   16.   ,    0.317,\n",
       "          60.   ,   25.   ,    5.   ]])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    #vectorizer = sklearn.feature_extraction.text.CountVectorizer(\n",
    "    tokenizer=tokenize,\n",
    "    ngram_range=(2, 3),\n",
    "    #stop_words=stopwords, #We do better when we keep stopwords\n",
    "    lowercase=True,\n",
    "    use_idf=True,\n",
    "    smooth_idf=True,\n",
    "    norm='l2',\n",
    "    decode_error='replace'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text = np.array(papers)\n",
    "text = pd.DataFrame(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    1\n",
       "2    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = pd.Series([0,1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results['text'] = text\n",
    "results['labels'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>labels</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22.0</td>\n",
       "      <td>6.51</td>\n",
       "      <td>12.96</td>\n",
       "      <td>14.6</td>\n",
       "      <td>18.81</td>\n",
       "      <td>26.9</td>\n",
       "      <td>16.963</td>\n",
       "      <td>6.563</td>\n",
       "      <td>156.0</td>\n",
       "      <td>6.122</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.310</td>\n",
       "      <td>73.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>The mid 2010s have witnessed a resurgence of n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.4</td>\n",
       "      <td>39.67</td>\n",
       "      <td>10.23</td>\n",
       "      <td>11.2</td>\n",
       "      <td>14.45</td>\n",
       "      <td>15.9</td>\n",
       "      <td>17.475</td>\n",
       "      <td>10.105</td>\n",
       "      <td>142.0</td>\n",
       "      <td>5.415</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.314</td>\n",
       "      <td>49.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1</td>\n",
       "      <td>What might a research program built on such a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18.2</td>\n",
       "      <td>11.25</td>\n",
       "      <td>12.10</td>\n",
       "      <td>15.9</td>\n",
       "      <td>19.61</td>\n",
       "      <td>21.5</td>\n",
       "      <td>16.427</td>\n",
       "      <td>3.760</td>\n",
       "      <td>132.0</td>\n",
       "      <td>6.311</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.317</td>\n",
       "      <td>60.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>All three objectives require scholars’ engagem...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0      1      2     3      4     5       6       7      8      9    10  \\\n",
       "0  22.0   6.51  12.96  14.6  18.81  26.9  16.963   6.563  156.0  6.122  13.0   \n",
       "1  13.4  39.67  10.23  11.2  14.45  15.9  17.475  10.105  142.0  5.415  15.0   \n",
       "2  18.2  11.25  12.10  15.9  19.61  21.5  16.427   3.760  132.0  6.311  16.0   \n",
       "\n",
       "      11    12    13   14  labels  \\\n",
       "0  0.310  73.0  16.0  4.0       0   \n",
       "1  0.314  49.0  16.0  6.0       1   \n",
       "2  0.317  60.0  25.0  5.0       0   \n",
       "\n",
       "                                                text  \n",
       "0  The mid 2010s have witnessed a resurgence of n...  \n",
       "1  What might a research program built on such a ...  \n",
       "2  All three objectives require scholars’ engagem...  "
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split into features and target\n",
    "y = results['labels']\n",
    "X_strings = results['text']\n",
    "vectorizer.fit(X_strings) # fit vectorizer here\n",
    "# X is our sparse matrix of predictors\n",
    "X = vectorizer.transform(X_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#MODELING\n",
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.1,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "bnb = naive_bayes.BernoulliNB()\n",
    "bnb.fit(X_train, y_train)\n",
    "y_pred = bnb.predict(X_test)\n",
    "\n",
    "# Evaluate this classifier! #\n",
    "# Precision: tp / (tp + fp)\n",
    "# Recall: tp / (tp + fn)\n",
    "# F_1: 2 * (precision * recall) / precision + recall\n",
    "#   e.g. harmonic mean of precision and recall\n",
    "print(\n",
    "    'Baseline guessing is {}'.format(float(sum(y_train)) / len(y_train))\n",
    ")\n",
    "print(\n",
    "    'The precision is {}'.format(metrics.precision_score(y_test, y_pred))\n",
    ")\n",
    "print(\n",
    "    'The recall is {}'.format(metrics.recall_score(y_test, y_pred))\n",
    ")\n",
    "print(\n",
    "    'The f score is {}'.format(metrics.f1_score(y_test, y_pred))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
